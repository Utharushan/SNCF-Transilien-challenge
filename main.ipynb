{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589fbd28",
   "metadata": {},
   "source": [
    "# SNCF Transilien: Fast Ensemble for p0q0 (MAE)\n",
    "\n",
    "This notebook trains a fast ensemble to predict p0q0 (difference between theoretical and realized waiting time) and exports `y_test.csv`. It prioritizes speed and competitive MAE using simple, leakage-safe features and an efficient blend of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee66a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data_dir: \\\\wsl.localhost\\Ubuntu-24.04\\home\\utharushan\\ChallengeData\\SNCF-Transilien-challenge\n",
      "Detected y_train file: y_train_final_j5KGWWK.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports and robust data_dir resolution\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LightGBM availability (optional fast model)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "# --- Robust path detection ---\n",
    "# Prefer forward-slash UNC to avoid backslash escaping issues on Windows\n",
    "candidate_dirs = [\n",
    "    Path('//wsl.localhost/Ubuntu-24.04/home/utharushan/ChallengeData/SNCF-Transilien-challenge'),\n",
    "    Path.cwd(),\n",
    "]\n",
    "\n",
    "required_files = {'x_train_final.csv', 'x_test_final.csv'}\n",
    "\n",
    "chosen: Path | None = None\n",
    "for cand in candidate_dirs:\n",
    "    try:\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            present = {p.name for p in cand.iterdir() if p.is_file()}\n",
    "            if required_files.issubset(present):\n",
    "                chosen = cand\n",
    "                break\n",
    "    except OSError:\n",
    "        # Inaccessible path (e.g., network/unmounted), skip\n",
    "        continue\n",
    "\n",
    "if chosen is None:\n",
    "    # If not all required files found, still try to pick a dir that at least exists\n",
    "    for cand in candidate_dirs:\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            chosen = cand\n",
    "            break\n",
    "\n",
    "if chosen is None:\n",
    "    raise FileNotFoundError(\n",
    "        'Aucun répertoire de données valide trouvé. Vérifiez le chemin des fichiers CSV. '\n",
    "        'Essayé: ' + ' | '.join(str(p) for p in candidate_dirs)\n",
    "    )\n",
    "\n",
    "# Ensure the two required files exist here\n",
    "missing = [fname for fname in required_files if not (chosen / fname).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Fichiers manquants dans {chosen}: {missing}.\\n\"\n",
    "        \"Assurez-vous que 'x_train_final.csv' et 'x_test_final.csv' sont bien présents.\"\n",
    "    )\n",
    "\n",
    "# Select y_train file by pattern\n",
    "y_candidates = sorted((p for p in chosen.glob('y_train*.csv')), key=lambda p: p.name)\n",
    "if not y_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Aucun fichier y_train* trouvé dans {chosen}. Placez le fichier de vérité terrain dans ce dossier.\"\n",
    "    )\n",
    "\n",
    "# Export variables used later\n",
    "data_dir = str(chosen)\n",
    "train_path = os.path.join(data_dir, 'x_train_final.csv')\n",
    "test_path = os.path.join(data_dir, 'x_test_final.csv')\n",
    "y_train_path = str(y_candidates[0])\n",
    "\n",
    "print(f\"Using data_dir: {data_dir}\")\n",
    "print(f\"Detected y_train file: {Path(y_train_path).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770053e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 0.81s: X=(667264, 12), X_test=(20657, 11), y=(667264, 2)\n",
      "Train columns: ['train', 'gare', 'date', 'arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4']\n",
      "Test columns: ['Unnamed: 0', 'train', 'gare', 'date', 'arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4']\n"
     ]
    }
   ],
   "source": [
    "# 2) Load Data (train/test) from CSV\n",
    "with Timer() as t:\n",
    "    X = pd.read_csv(train_path)\n",
    "    X_test = pd.read_csv(test_path)\n",
    "    y = pd.read_csv(y_train_path)\n",
    "print(f\"Loaded data in {t.dt:.2f}s: X={X.shape}, X_test={X_test.shape}, y={y.shape}\")\n",
    "\n",
    "# Align and clean columns\n",
    "# Remove potential unnamed index columns\n",
    "X = X.loc[:, ~X.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# y column may be named p0q0 or similar; ensure it's named 'p0q0'\n",
    "if y.shape[1] == 1:\n",
    "    y.columns = ['p0q0']\n",
    "else:\n",
    "    # If there's an index + a single target col, take the last\n",
    "    y = y.iloc[:, [-1]]\n",
    "    y.columns = ['p0q0']\n",
    "\n",
    "# Coerce dtypes\n",
    "for col in ['train', 'gare']:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].astype(str)\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "if 'date' in X.columns:\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "if 'date' in X_test.columns:\n",
    "    X_test['date'] = pd.to_datetime(X_test['date'])\n",
    "\n",
    "if 'arret' in X.columns:\n",
    "    X['arret'] = X['arret'].astype(int)\n",
    "if 'arret' in X_test.columns:\n",
    "    X_test['arret'] = X_test['arret'].astype(int)\n",
    "\n",
    "lag_cols = ['p2q0','p3q0','p4q0','p0q2','p0q3','p0q4']\n",
    "for c in lag_cols:\n",
    "    if c in X.columns:\n",
    "        X[c] = X[c].astype(float)\n",
    "    if c in X_test.columns:\n",
    "        X_test[c] = X_test[c].astype(float)\n",
    "\n",
    "# Ensure train/test columns match except target\n",
    "feature_cols = [c for c in X.columns if c != 'p0q0']\n",
    "print(\"Train columns:\", feature_cols)\n",
    "print(\"Test columns:\", list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df5e336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train gare       date  arret  p2q0  p3q0  p4q0  p0q2  p0q3  p0q4\n",
      "0  VBXNMF  KYF 2023-04-03      8   0.0   0.0   1.0  -3.0  -1.0  -2.0\n",
      "1  VBXNMF  JLR 2023-04-03      9   0.0   0.0   0.0   1.0   0.0   1.0\n",
      "2  VBXNMF  EOH 2023-04-03     10  -1.0   0.0   0.0  -1.0   0.0   0.0\n",
      "   Unnamed: 0   train gare       date  arret  p2q0  p3q0  p4q0  p0q2  p0q3  p0q4\n",
      "0           0  ZPQEKP  VXY 2023-11-13     12   0.0   0.0  -2.0  -4.0  -2.0  -4.0\n",
      "1           1  KIQSRA  VXY 2023-11-13     12   0.0   0.0  -1.0   1.0  -1.0   0.0\n",
      "2           2  QQJYYT  VXY 2023-11-13     12   0.0   1.0  -1.0   1.0  -1.0   1.0\n",
      "y head:\n",
      "    p0q0\n",
      "0  -1.0\n",
      "1  -1.0\n",
      "2  -1.0\n",
      "3   1.0\n",
      "4   3.0\n",
      "Missing in X:\n",
      " train    0\n",
      "gare     0\n",
      "date     0\n",
      "arret    0\n",
      "p2q0     0\n",
      "p3q0     0\n",
      "p4q0     0\n",
      "p0q2     0\n",
      "p0q3     0\n",
      "p0q4     0\n",
      "dtype: int64\n",
      "Missing in X_test:\n",
      " Unnamed: 0    0\n",
      "train         0\n",
      "gare          0\n",
      "date          0\n",
      "arret         0\n",
      "p2q0          0\n",
      "p3q0          0\n",
      "p4q0          0\n",
      "p0q2          0\n",
      "p0q3          0\n",
      "p0q4          0\n",
      "dtype: int64\n",
      "Unique train: train=37544 test=8036\n",
      "Unique gare: train=84 test=81\n",
      "Date range train: 2023-04-03 00:00:00 2023-11-10 00:00:00\n",
      "Date range test: 2023-11-13 00:00:00 2023-12-22 00:00:00\n",
      "Target stats: count    667264.000000\n",
      "mean         -0.159950\n",
      "std           1.987872\n",
      "min        -160.000000\n",
      "25%          -1.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max          15.000000\n",
      "Name: p0q0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3) Basic Data Checks and Sanity Validations\n",
    "print(X.head(3))\n",
    "print(X_test.head(3))\n",
    "print(\"y head:\\n\", y.head())\n",
    "\n",
    "print(\"Missing in X:\\n\", X.isna().sum())\n",
    "print(\"Missing in X_test:\\n\", X_test.isna().sum())\n",
    "\n",
    "# Sanity: ensure target length matches X rows\n",
    "assert len(y) == len(X), (len(y), len(X))\n",
    "\n",
    "# Unique counts\n",
    "for col in ['train','gare']:\n",
    "    if col in X.columns:\n",
    "        print(f\"Unique {col}: train={X[col].nunique()} test={X_test[col].nunique()}\")\n",
    "\n",
    "if 'date' in X.columns:\n",
    "    print(\"Date range train:\", X['date'].min(), X['date'].max())\n",
    "    print(\"Date range test:\", X_test['date'].min(), X_test['date'].max())\n",
    "\n",
    "print('Target stats:', y['p0q0'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c985752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering in 2.65s\n",
      "Numeric cols ( 87 ): ['arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4', 'year', 'month', 'day', 'weekday', 'weekofyear', 'dayofyear', 'p2q0_mean_x', 'p2q0_median_x', 'p2q0_std_x', 'p3q0_mean_x', 'p3q0_median_x', 'p3q0_std_x', 'p4q0_mean_x'] ...\n",
      "Categorical cols ( 2 ): ['train', 'gare']\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature engineering helpers and application\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def Timer(msg: str):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{msg} in {time.time()-t0:.2f}s\")\n",
    "\n",
    "def add_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d['year'] = d['date'].dt.year.astype('int16')\n",
    "    d['month'] = d['date'].dt.month.astype('int8')\n",
    "    d['day'] = d['date'].dt.day.astype('int8')\n",
    "    d['weekday'] = d['date'].dt.weekday.astype('int8')\n",
    "    d['weekofyear'] = d['date'].dt.isocalendar().week.astype('int16')\n",
    "    d['dayofyear'] = d['date'].dt.dayofyear.astype('int16')\n",
    "    # Cyclical encodings for seasonality\n",
    "    for col, mod in [('month', 12), ('weekday', 7), ('weekofyear', 53)]:\n",
    "        ang = 2 * np.pi * (d[col].astype(float) / mod)\n",
    "        d[f'{col}_sin'] = np.sin(ang).astype('float32')\n",
    "        d[f'{col}_cos'] = np.cos(ang).astype('float32')\n",
    "    return d\n",
    "\n",
    "def add_gare_aggregates(train_df: pd.DataFrame, test_df: pd.DataFrame, keys=('gare',)):\n",
    "    # Leakage-safe aggregates computed only from train\n",
    "    cols = ['p2q0','p3q0','p4q0','p0q2','p0q3','p0q4']\n",
    "    aggs = {}\n",
    "    for c in cols:\n",
    "        aggs[c] = ['mean','median','std','min','max']\n",
    "    agg_df = train_df.groupby(list(keys))[cols].agg(aggs)\n",
    "    agg_df.columns = ['_'.join([c, stat]) for c, stat in agg_df.columns]\n",
    "    agg_df = agg_df.reset_index()\n",
    "    for df in (train_df, test_df):\n",
    "        df = df.merge(agg_df, on=list(keys), how='left')\n",
    "    return train_df.merge(agg_df, on=list(keys), how='left'), test_df.merge(agg_df, on=list(keys), how='left'), agg_df\n",
    "\n",
    "def add_lag_interactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    lags = ['p2q0','p3q0','p4q0','p0q2','p0q3','p0q4']\n",
    "    # Absolute values and signs\n",
    "    for c in lags:\n",
    "        d[f'{c}_abs'] = d[c].abs().astype('float32')\n",
    "        d[f'{c}_sign'] = np.sign(d[c]).astype('int8')\n",
    "    # Pairwise sums and differences (limited to a few to avoid explosion)\n",
    "    pairs = [('p2q0','p3q0'), ('p3q0','p4q0'), ('p0q2','p0q3'), ('p0q3','p0q4')]\n",
    "    for a,b in pairs:\n",
    "        d[f'{a}_plus_{b}'] = (d[a] + d[b]).astype('float32')\n",
    "        d[f'{a}_minus_{b}'] = (d[a] - d[b]).astype('float32')\n",
    "    return d\n",
    "\n",
    "with Timer('Feature engineering'):\n",
    "    X_fe = add_date_features(X)\n",
    "    X_test_fe = add_date_features(X_test)\n",
    "    X_fe, X_test_fe, agg_df = add_gare_aggregates(X_fe, X_test_fe, keys=('gare',))\n",
    "    X_fe = add_lag_interactions(X_fe)\n",
    "    X_test_fe = add_lag_interactions(X_test_fe)\n",
    "\n",
    "# Build feature/target matrices\n",
    "all_cols = [c for c in X_fe.columns if c not in ['date']]\n",
    "feature_cols = [c for c in all_cols if c != 'p0q0']\n",
    "lag_cols = ['p2q0','p3q0','p4q0','p0q2','p0q3','p0q4']\n",
    "\n",
    "categorical_cols = ['train','gare']\n",
    "numeric_cols = [c for c in feature_cols if c not in categorical_cols]\n",
    "\n",
    "print(f\"Numeric cols ( {len(numeric_cols)} ): {numeric_cols[:20]} ...\")\n",
    "print(f\"Categorical cols ( {len(categorical_cols)} ): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e343b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing fit+transform in 14.18s\n",
      "Split: time-based split by date, valid_date=2023-11-10\n",
      "Shapes: tr=(660238, 89), va=(7026, 89), full=(667264, 89), test=(20657, 89)\n"
     ]
    }
   ],
   "source": [
    "# 4) Preprocessing and date-aware split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Update preprocessing with new features\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Use the last day as validation; also prepare for CV later\n",
    "last_date = X_fe['date'].max()\n",
    "valid_mask = X_fe['date'] == last_date\n",
    "train_mask = ~valid_mask\n",
    "\n",
    "X_tr, X_va = X_fe.loc[train_mask, feature_cols], X_fe.loc[valid_mask, feature_cols]\n",
    "y_tr, y_va = y.loc[train_mask, 'p0q0'].values, y.loc[valid_mask, 'p0q0'].values\n",
    "\n",
    "# No downsampling now; we go full training for stronger models\n",
    "with Timer('Preprocessing fit+transform'):\n",
    "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
    "    Xt_va = preprocessor.transform(X_va)\n",
    "    Xt_full = preprocessor.fit_transform(X_fe[feature_cols])\n",
    "    Xt_test = preprocessor.transform(X_test_fe[feature_cols])\n",
    "\n",
    "print(\n",
    "    f\"Split: time-based split by date, valid_date={last_date.date()}\\n\"\n",
    "    f\"Shapes: tr={Xt_tr.shape}, va={Xt_va.shape}, full={Xt_full.shape}, test={Xt_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780d6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled training split to 250,000 rows for speed\n",
      "Split: time-based split by date, valid_date=2023-11-10\n",
      "Preprocessing fit+transform in 2.80s; Shapes: tr=(250000, 33), va=(7026, 33), full=(667264, 33), test=(20657, 33)\n",
      "Preprocessing fit+transform in 2.80s; Shapes: tr=(250000, 33), va=(7026, 33), full=(667264, 33), test=(20657, 33)\n"
     ]
    }
   ],
   "source": [
    "# 6) Train/Validation Split (date-aware)\n",
    "# If multiple dates, use the latest date as validation; else k-fold fallback\n",
    "if 'date' in X.columns and X['date'].nunique() > 1:\n",
    "    last_date = X['date'].max()\n",
    "    train_mask = X['date'] < last_date\n",
    "    valid_mask = X['date'] == last_date\n",
    "    X_tr, X_va = X.loc[train_mask].copy(), X.loc[valid_mask].copy()\n",
    "    y_tr, y_va = y.loc[train_mask, 'p0q0'].values, y.loc[valid_mask, 'p0q0'].values\n",
    "    split_desc = f\"time-based split by date, valid_date={last_date.date()}\"\n",
    "else:\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X, y['p0q0'].values, test_size=0.1, random_state=RANDOM_STATE)\n",
    "    split_desc = \"random 90/10 split (no multiple dates)\"\n",
    "\n",
    "# Optional downsampling of training rows for speed if extremely large\n",
    "MAX_TRAIN_ROWS = 250_000  # cap to ensure quick training while keeping good signal\n",
    "if len(X_tr) > MAX_TRAIN_ROWS:\n",
    "    idx = np.random.RandomState(RANDOM_STATE).choice(len(X_tr), size=MAX_TRAIN_ROWS, replace=False)\n",
    "    X_tr = X_tr.iloc[idx]\n",
    "    y_tr = y_tr[idx]\n",
    "    print(f\"Downsampled training split to {len(X_tr):,} rows for speed\")\n",
    "\n",
    "print(\"Split:\", split_desc)\n",
    "\n",
    "with Timer() as t:\n",
    "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
    "    Xt_va = preprocessor.transform(X_va)\n",
    "    Xt_full = preprocessor.fit_transform(X)  # for refit later\n",
    "    Xt_test = preprocessor.transform(X_test)\n",
    "print(f\"Preprocessing fit+transform in {t.dt:.2f}s; Shapes: tr={Xt_tr.shape}, va={Xt_va.shape}, full={Xt_full.shape}, test={Xt_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "052093a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['hgb', 'rf', 'et', 'ridge', 'lgbm']\n"
     ]
    }
   ],
   "source": [
    "# 6) Define models (stronger configs) and containers\n",
    "models = []\n",
    "# HistGradientBoosting tuned for MAE\n",
    "models.append(('hgb', HistGradientBoostingRegressor(\n",
    "    loss='absolute_error', learning_rate=0.05, max_depth=7, max_iter=400,\n",
    "    l2_regularization=0.0, early_stopping=True, random_state=RANDOM_STATE\n",
    ")))\n",
    "# RandomForest, deeper with more trees\n",
    "models.append(('rf', RandomForestRegressor(\n",
    "    n_estimators=600, max_depth=18, min_samples_leaf=1, n_jobs=-1,\n",
    "    bootstrap=True, random_state=RANDOM_STATE\n",
    ")))\n",
    "# ExtraTrees, strong\n",
    "models.append(('et', ExtraTreesRegressor(\n",
    "    n_estimators=600, max_depth=22, min_samples_leaf=1, n_jobs=-1,\n",
    "    bootstrap=False, random_state=RANDOM_STATE\n",
    ")))\n",
    "# Ridge as a linear baseline on engineered features\n",
    "models.append(('ridge', Ridge(alpha=1.0, random_state=RANDOM_STATE)))\n",
    "\n",
    "# Optional LightGBM\n",
    "if HAS_LGBM:\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective='mae',\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=3000,\n",
    "        max_depth=-1,\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    models.append(('lgbm', model))\n",
    "\n",
    "model_names = [name for name, _ in models]\n",
    "print('Models:', model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f37be936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15632\n",
      "[LightGBM] [Info] Number of data points in the train set: 220080, number of used features: 88\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15632\n",
      "[LightGBM] [Info] Number of data points in the train set: 220080, number of used features: 88\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26711\n",
      "[LightGBM] [Info] Number of data points in the train set: 440159, number of used features: 88\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26711\n",
      "[LightGBM] [Info] Number of data points in the train set: 440159, number of used features: 88\n",
      "2-fold time CV (HGB+LGBM) in 427.46s\n",
      "Model hgb val MAE: 1.5915\n",
      "Model lgbm val MAE: 1.5915\n",
      "2-fold time CV (HGB+LGBM) in 427.46s\n",
      "Model hgb val MAE: 1.5915\n",
      "Model lgbm val MAE: 1.5915\n"
     ]
    }
   ],
   "source": [
    "# 7) Time-based CV (lighter): only HGB + LGBM, 2 folds, OOF + test averaging\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Keep CV light: only strongest, fastest models\n",
    "cv_model_names = [name for name, _ in models if name in ('hgb', 'lgbm') and (name != 'lgbm' or HAS_LGBM)]\n",
    "\n",
    "n_splits = 2\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "val_preds = {name: np.zeros_like(y_va, dtype=float) for name in cv_model_names}\n",
    "P = {name: np.zeros(Xt_full.shape[0], dtype=float) for name in cv_model_names}\n",
    "Ptest = {name: np.zeros(Xt_test.shape[0], dtype=float) for name in cv_model_names}\n",
    "\n",
    "# Prepare global categorical categories to ensure matching across train/valid/test (for LGBM)\n",
    "cat_categories = {}\n",
    "for c in categorical_cols:\n",
    "    cats = pd.Categorical(pd.concat([\n",
    "        X_fe[c].astype('string'),\n",
    "        X_test_fe[c].astype('string')\n",
    "    ], ignore_index=True)).categories\n",
    "    cat_categories[c] = cats\n",
    "\n",
    "def set_cats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    for c in categorical_cols:\n",
    "        d[c] = pd.Categorical(d[c].astype('string'), categories=cat_categories[c], ordered=False)\n",
    "    return d\n",
    "\n",
    "# Fit on full matrix, but we’ll create folds using training rows indices\n",
    "train_idx = np.where(train_mask.values)[0]\n",
    "valid_idx = np.where(valid_mask.values)[0]\n",
    "\n",
    "with Timer(f'{n_splits}-fold time CV (HGB+LGBM)'):\n",
    "    for fold, (tr_idx_rel, va_idx_rel) in enumerate(cv.split(Xt_tr), 1):\n",
    "        tr_idx = train_idx[tr_idx_rel]\n",
    "        va_idx = train_idx[va_idx_rel]\n",
    "        Xtr, Xva = Xt_full[tr_idx], Xt_full[va_idx]\n",
    "        ytr, yva = y['p0q0'].values[tr_idx], y['p0q0'].values[va_idx]\n",
    "        for name, model in models:\n",
    "            if name not in cv_model_names:\n",
    "                continue\n",
    "            if name == 'lgbm' and HAS_LGBM:\n",
    "                # Fit LightGBM on raw engineered pandas with consistent categories\n",
    "                train_pd = set_cats(X_fe.iloc[tr_idx][feature_cols])\n",
    "                valid_pd = set_cats(X_fe.iloc[va_idx][feature_cols])\n",
    "                test_pd = set_cats(X_test_fe[feature_cols])\n",
    "                model.set_params(n_estimators=2000)  # keep CV fast\n",
    "                model.fit(\n",
    "                    train_pd, ytr,\n",
    "                    eval_set=[(valid_pd, yva)],\n",
    "                    eval_metric='l1',\n",
    "                    categorical_feature=categorical_cols,\n",
    "                    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "                )\n",
    "                pv = model.predict(valid_pd)\n",
    "                pt = model.predict(test_pd)\n",
    "            else:\n",
    "                # Sklearn path using preprocessed arrays\n",
    "                model.fit(Xtr, ytr)\n",
    "                pv = model.predict(Xva)\n",
    "                pt = model.predict(Xt_test)\n",
    "            P[name][va_idx] = pv\n",
    "            Ptest[name] += pt / n_splits\n",
    "\n",
    "# Validation slice predictions (last-date indices)\n",
    "for name in cv_model_names:\n",
    "    val_preds[name] = P[name][valid_idx]\n",
    "    print(f\"Model {name} val MAE: {mean_absolute_error(y_va, val_preds[name]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a585b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ensemble MAE= 1.5915\n",
      "Selected weights: {'hgb': 0.0, 'lgbm': 1.0}\n",
      "MAE definition: MAE = (1/n) * sum(|y_i - yhat_i|)\n",
      "Meta-learner (Huber) MAE on val: 1.6300\n"
     ]
    }
   ],
   "source": [
    "# 8) Blend OOF/validation predictions: simplex grid + meta stacking (optional)\n",
    "from itertools import product\n",
    "\n",
    "# Restrict to models actually validated\n",
    "order = list(val_preds.keys())\n",
    "mae = lambda w: mean_absolute_error(y_va, sum(w[i]*val_preds[name] for i, name in enumerate(order)))\n",
    "\n",
    "best_w = None\n",
    "best_mae = 1e9\n",
    "step = 0.1\n",
    "\n",
    "# Simplex grid (sum to 1)\n",
    "for w in product(np.arange(0, 1+1e-9, step), repeat=len(order)):\n",
    "    if abs(sum(w) - 1.0) > 1e-6:\n",
    "        continue\n",
    "    m = mae(w)\n",
    "    if m < best_mae:\n",
    "        best_mae = m\n",
    "        best_w = np.array(w)\n",
    "\n",
    "print('Best ensemble MAE=', round(best_mae, 4))\n",
    "print('Selected weights:', {name: float(best_w[i]) for i, name in enumerate(order)})\n",
    "print('MAE definition: MAE = (1/n) * sum(|y_i - yhat_i|)')\n",
    "\n",
    "# Optional: simple meta-learner on validation slice\n",
    "try:\n",
    "    from sklearn.linear_model import HuberRegressor\n",
    "    meta = HuberRegressor(alpha=0.0005)\n",
    "    Z_va = np.column_stack([val_preds[name] for name in order])\n",
    "    meta.fit(Z_va, y_va)\n",
    "    meta_pred = meta.predict(Z_va)\n",
    "    meta_mae = mean_absolute_error(y_va, meta_pred)\n",
    "    print(f'Meta-learner (Huber) MAE on val: {meta_mae:.4f}')\n",
    "    use_meta = meta_mae < best_mae\n",
    "except Exception:\n",
    "    use_meta = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b937535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35442\n",
      "[LightGBM] [Info] Number of data points in the train set: 667264, number of used features: 88\n",
      "[LightGBM] [Warning] categorical_feature is set=0,1, categorical_column=0,1 will be ignored. Current value: categorical_feature=0,1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35442\n",
      "[LightGBM] [Info] Number of data points in the train set: 667264, number of used features: 88\n",
      "Contributing members: ['lgbm']\n",
      "Contributing members: ['lgbm']\n"
     ]
    }
   ],
   "source": [
    "# 9) Refit contributing models on full data and predict test (aligned with selection)\n",
    "fit_models_full = {}\n",
    "preds_test_members = {}\n",
    "\n",
    "if use_meta:\n",
    "    Z_test = np.column_stack([Ptest[name] for name in order])\n",
    "    meta_full = HuberRegressor(alpha=0.0005)\n",
    "    meta_full.fit(Z_va, y_va)\n",
    "    y_test_pred = meta_full.predict(Z_test)\n",
    "    preds_test_members = {name: Ptest[name] for name in order}\n",
    "else:\n",
    "    contrib = [name for i,name in enumerate(order) if best_w[i] > 0]\n",
    "    for name, model in models:\n",
    "        if name not in contrib:\n",
    "            continue\n",
    "        if name == 'lgbm' and HAS_LGBM:\n",
    "            train_pd = set_cats(X_fe[feature_cols].copy())\n",
    "            model.fit(train_pd, y['p0q0'].values, categorical_feature=categorical_cols)\n",
    "            test_pd = set_cats(X_test_fe[feature_cols].copy())\n",
    "            preds_test_members[name] = model.predict(test_pd)\n",
    "        elif name == 'hgb':\n",
    "            model.fit(Xt_full, y['p0q0'].values)\n",
    "            preds_test_members[name] = model.predict(Xt_test)\n",
    "    y_test_pred = np.zeros(Xt_test.shape[0], dtype=float)\n",
    "    for i, name in enumerate(order):\n",
    "        if best_w[i] > 0:\n",
    "            y_test_pred += best_w[i] * preds_test_members[name]\n",
    "\n",
    "print('Contributing members:', [k for k in preds_test_members.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42fd66e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred range before rounding: [-4.08, 2.98] -> after: [-4, 3] with clip to [-160,15]\n"
     ]
    }
   ],
   "source": [
    "# 10) Post-processing and prepare submission\n",
    "# Round to nearest int; clip to observed training range\n",
    "y_min, y_max = int(np.floor(y['p0q0'].min())), int(np.ceil(y['p0q0'].max()))\n",
    "y_submit = np.rint(y_test_pred).astype(int)\n",
    "do_clip = True\n",
    "if do_clip:\n",
    "    y_submit = np.clip(y_submit, y_min, y_max)\n",
    "\n",
    "print(f\"Pred range before rounding: [{y_test_pred.min():.2f}, {y_test_pred.max():.2f}] -> after: [{y_submit.min()}, {y_submit.max()}] with clip to [{y_min},{y_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93bddb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to: \\\\wsl.localhost\\Ubuntu-24.04\\home\\utharushan\\ChallengeData\\SNCF-Transilien-challenge\\y_test.csv\n",
      "   p0q0\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n"
     ]
    }
   ],
   "source": [
    "# 12) Write Submission File y_test.csv\n",
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(data_dir) / 'y_test.csv'\n",
    "sub = pd.DataFrame({'p0q0': y_submit.astype(int)})\n",
    "sub.to_csv(out_path, index=True, index_label='id')\n",
    "print(f\"Wrote submission to: {out_path}\")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid MAE hgb: 1.4205\n",
      "Valid MAE rf: 1.7267\n",
      "Valid MAE et: 1.5084\n",
      "Valid MAE ridge: 1.5388\n",
      "Valid MAE lgbm: 1.3514\n",
      "Ensemble MAE: 1.3514\n",
      "Weights: {'hgb': 0.0, 'rf': 0.0, 'et': 0.0, 'ridge': 0.0, 'lgbm': 1.0}\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 13) Logs\n",
    "for name in model_names:\n",
    "    if name in val_preds:\n",
    "        print(f\"Valid MAE {name}: {mean_absolute_error(y_va, val_preds[name]):.4f}\")\n",
    "print(f\"Ensemble MAE: {best_mae:.4f}\")\n",
    "print(f\"Weights: {{name: float(best_w[i]) for i, name in enumerate(model_names)}}\")\n",
    "print('Meta used:', use_meta)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
