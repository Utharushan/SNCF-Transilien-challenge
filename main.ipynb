{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589fbd28",
   "metadata": {},
   "source": [
    "# SNCF Transilien: Fast Ensemble for p0q0 (MAE)\n",
    "\n",
    "This notebook trains a fast ensemble to predict p0q0 (difference between theoretical and realized waiting time) and exports `y_test.csv`. It prioritizes speed and competitive MAE using simple, leakage-safe features and an efficient blend of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee66a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data_dir: \\\\wsl.localhost\\Ubuntu-24.04\\home\\utharushan\\ChallengeData\\SNCF-Transilien-challenge\n",
      "Detected y_train file: y_train_final_j5KGWWK.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports and robust data_dir resolution\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LightGBM availability (optional fast model)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "# --- Robust path detection ---\n",
    "# Prefer forward-slash UNC to avoid backslash escaping issues on Windows\n",
    "candidate_dirs = [\n",
    "    Path('//wsl.localhost/Ubuntu-24.04/home/utharushan/ChallengeData/SNCF-Transilien-challenge'),\n",
    "    Path.cwd(),\n",
    "]\n",
    "\n",
    "required_files = {'x_train_final.csv', 'x_test_final.csv'}\n",
    "\n",
    "chosen: Path | None = None\n",
    "for cand in candidate_dirs:\n",
    "    try:\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            present = {p.name for p in cand.iterdir() if p.is_file()}\n",
    "            if required_files.issubset(present):\n",
    "                chosen = cand\n",
    "                break\n",
    "    except OSError:\n",
    "        # Inaccessible path (e.g., network/unmounted), skip\n",
    "        continue\n",
    "\n",
    "if chosen is None:\n",
    "    # If not all required files found, still try to pick a dir that at least exists\n",
    "    for cand in candidate_dirs:\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            chosen = cand\n",
    "            break\n",
    "\n",
    "if chosen is None:\n",
    "    raise FileNotFoundError(\n",
    "        'Aucun répertoire de données valide trouvé. Vérifiez le chemin des fichiers CSV. '\n",
    "        'Essayé: ' + ' | '.join(str(p) for p in candidate_dirs)\n",
    "    )\n",
    "\n",
    "# Ensure the two required files exist here\n",
    "missing = [fname for fname in required_files if not (chosen / fname).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Fichiers manquants dans {chosen}: {missing}.\\n\"\n",
    "        \"Assurez-vous que 'x_train_final.csv' et 'x_test_final.csv' sont bien présents.\"\n",
    "    )\n",
    "\n",
    "# Select y_train file by pattern\n",
    "y_candidates = sorted((p for p in chosen.glob('y_train*.csv')), key=lambda p: p.name)\n",
    "if not y_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Aucun fichier y_train* trouvé dans {chosen}. Placez le fichier de vérité terrain dans ce dossier.\"\n",
    "    )\n",
    "\n",
    "# Export variables used later\n",
    "data_dir = str(chosen)\n",
    "train_path = os.path.join(data_dir, 'x_train_final.csv')\n",
    "test_path = os.path.join(data_dir, 'x_test_final.csv')\n",
    "y_train_path = str(y_candidates[0])\n",
    "\n",
    "print(f\"Using data_dir: {data_dir}\")\n",
    "print(f\"Detected y_train file: {Path(y_train_path).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770053e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 0.81s: X=(667264, 12), X_test=(20657, 11), y=(667264, 2)\n",
      "Train columns: ['train', 'gare', 'date', 'arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4']\n",
      "Test columns: ['Unnamed: 0', 'train', 'gare', 'date', 'arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4']\n"
     ]
    }
   ],
   "source": [
    "# 2) Load Data (train/test) from CSV\n",
    "with Timer() as t:\n",
    "    X = pd.read_csv(train_path)\n",
    "    X_test = pd.read_csv(test_path)\n",
    "    y = pd.read_csv(y_train_path)\n",
    "print(f\"Loaded data in {t.dt:.2f}s: X={X.shape}, X_test={X_test.shape}, y={y.shape}\")\n",
    "\n",
    "# Align and clean columns\n",
    "# Remove potential unnamed index columns\n",
    "X = X.loc[:, ~X.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# y column may be named p0q0 or similar; ensure it's named 'p0q0'\n",
    "if y.shape[1] == 1:\n",
    "    y.columns = ['p0q0']\n",
    "else:\n",
    "    # If there's an index + a single target col, take the last\n",
    "    y = y.iloc[:, [-1]]\n",
    "    y.columns = ['p0q0']\n",
    "\n",
    "# Coerce dtypes\n",
    "for col in ['train', 'gare']:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].astype(str)\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "if 'date' in X.columns:\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "if 'date' in X_test.columns:\n",
    "    X_test['date'] = pd.to_datetime(X_test['date'])\n",
    "\n",
    "if 'arret' in X.columns:\n",
    "    X['arret'] = X['arret'].astype(int)\n",
    "if 'arret' in X_test.columns:\n",
    "    X_test['arret'] = X_test['arret'].astype(int)\n",
    "\n",
    "lag_cols = ['p2q0','p3q0','p4q0','p0q2','p0q3','p0q4']\n",
    "for c in lag_cols:\n",
    "    if c in X.columns:\n",
    "        X[c] = X[c].astype(float)\n",
    "    if c in X_test.columns:\n",
    "        X_test[c] = X_test[c].astype(float)\n",
    "\n",
    "# Ensure train/test columns match except target\n",
    "feature_cols = [c for c in X.columns if c != 'p0q0']\n",
    "print(\"Train columns:\", feature_cols)\n",
    "print(\"Test columns:\", list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df5e336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train gare       date  arret  p2q0  p3q0  p4q0  p0q2  p0q3  p0q4\n",
      "0  VBXNMF  KYF 2023-04-03      8   0.0   0.0   1.0  -3.0  -1.0  -2.0\n",
      "1  VBXNMF  JLR 2023-04-03      9   0.0   0.0   0.0   1.0   0.0   1.0\n",
      "2  VBXNMF  EOH 2023-04-03     10  -1.0   0.0   0.0  -1.0   0.0   0.0\n",
      "   Unnamed: 0   train gare       date  arret  p2q0  p3q0  p4q0  p0q2  p0q3  p0q4\n",
      "0           0  ZPQEKP  VXY 2023-11-13     12   0.0   0.0  -2.0  -4.0  -2.0  -4.0\n",
      "1           1  KIQSRA  VXY 2023-11-13     12   0.0   0.0  -1.0   1.0  -1.0   0.0\n",
      "2           2  QQJYYT  VXY 2023-11-13     12   0.0   1.0  -1.0   1.0  -1.0   1.0\n",
      "y head:\n",
      "    p0q0\n",
      "0  -1.0\n",
      "1  -1.0\n",
      "2  -1.0\n",
      "3   1.0\n",
      "4   3.0\n",
      "Missing in X:\n",
      " train    0\n",
      "gare     0\n",
      "date     0\n",
      "arret    0\n",
      "p2q0     0\n",
      "p3q0     0\n",
      "p4q0     0\n",
      "p0q2     0\n",
      "p0q3     0\n",
      "p0q4     0\n",
      "dtype: int64\n",
      "Missing in X_test:\n",
      " Unnamed: 0    0\n",
      "train         0\n",
      "gare          0\n",
      "date          0\n",
      "arret         0\n",
      "p2q0          0\n",
      "p3q0          0\n",
      "p4q0          0\n",
      "p0q2          0\n",
      "p0q3          0\n",
      "p0q4          0\n",
      "dtype: int64\n",
      "Unique train: train=37544 test=8036\n",
      "Unique gare: train=84 test=81\n",
      "Date range train: 2023-04-03 00:00:00 2023-11-10 00:00:00\n",
      "Date range test: 2023-11-13 00:00:00 2023-12-22 00:00:00\n",
      "Target stats: count    667264.000000\n",
      "mean         -0.159950\n",
      "std           1.987872\n",
      "min        -160.000000\n",
      "25%          -1.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max          15.000000\n",
      "Name: p0q0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3) Basic Data Checks and Sanity Validations\n",
    "print(X.head(3))\n",
    "print(X_test.head(3))\n",
    "print(\"y head:\\n\", y.head())\n",
    "\n",
    "print(\"Missing in X:\\n\", X.isna().sum())\n",
    "print(\"Missing in X_test:\\n\", X_test.isna().sum())\n",
    "\n",
    "# Sanity: ensure target length matches X rows\n",
    "assert len(y) == len(X), (len(y), len(X))\n",
    "\n",
    "# Unique counts\n",
    "for col in ['train','gare']:\n",
    "    if col in X.columns:\n",
    "        print(f\"Unique {col}: train={X[col].nunique()} test={X_test[col].nunique()}\")\n",
    "\n",
    "if 'date' in X.columns:\n",
    "    print(\"Date range train:\", X['date'].min(), X['date'].max())\n",
    "    print(\"Date range test:\", X_test['date'].min(), X_test['date'].max())\n",
    "\n",
    "print('Target stats:', y['p0q0'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c985752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering done in 0.44s\n"
     ]
    }
   ],
   "source": [
    "# 4) Feature Engineering (dates, simple aggregates)\n",
    "with Timer() as t:\n",
    "    def add_date_features(df):\n",
    "        if 'date' not in df.columns:\n",
    "            return df\n",
    "        df = df.copy()\n",
    "        df['year'] = df['date'].dt.year.astype(int)\n",
    "        df['month'] = df['date'].dt.month.astype(int)\n",
    "        df['day'] = df['date'].dt.day.astype(int)\n",
    "        df['weekday'] = df['date'].dt.weekday.astype(int)\n",
    "        df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "        df['dayofyear'] = df['date'].dt.dayofyear.astype(int)\n",
    "        return df\n",
    "\n",
    "    X = add_date_features(X)\n",
    "    X_test = add_date_features(X_test)\n",
    "\n",
    "    # Simple, leakage-safe aggregates from existing lag features\n",
    "    agg_keys = ['gare'] if 'gare' in X.columns else []\n",
    "    agg_features = {}\n",
    "    for c in lag_cols:\n",
    "        if c in X.columns:\n",
    "            agg_features[c] = ['mean','median','std']\n",
    "\n",
    "    if agg_keys and agg_features:\n",
    "        agg_df = (X.groupby(agg_keys)[list(agg_features.keys())]\n",
    "                    .agg(agg_features))\n",
    "        # flatten columns\n",
    "        agg_df.columns = [f\"{a}_{b}\" for a,b in agg_df.columns]\n",
    "        agg_df = agg_df.reset_index()\n",
    "        # fillna with global means to speedup join handling\n",
    "        agg_df = agg_df.fillna(agg_df.mean(numeric_only=True))\n",
    "\n",
    "        X = X.merge(agg_df, on=agg_keys, how='left')\n",
    "        X_test = X_test.merge(agg_df, on=agg_keys, how='left')\n",
    "\n",
    "        # For unseen keys in test, fill with global means from train\n",
    "        for col in agg_df.columns:\n",
    "            if col in X_test.columns:\n",
    "                if X_test[col].isna().any():\n",
    "                    fill_val = X[col].mean() if col in X.columns else 0.0\n",
    "                    X_test[col] = X_test[col].fillna(fill_val)\n",
    "\n",
    "print(f\"Feature engineering done in {t.dt:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e343b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols ( 31 ): ['arret', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4', 'year', 'month', 'day', 'weekday', 'weekofyear', 'dayofyear', 'p2q0_mean', 'p2q0_median', 'p2q0_std', 'p3q0_mean', 'p3q0_median', 'p3q0_std', 'p4q0_mean'] ...\n",
      "Categorical cols ( 2 ): ['train', 'gare']\n"
     ]
    }
   ],
   "source": [
    "# 5) Build Preprocessing Pipeline (ColumnTransformer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "all_cols = [c for c in X.columns]\n",
    "\n",
    "categorical_cols = [c for c in ['train','gare'] if c in X.columns]\n",
    "drop_cols = []\n",
    "if 'date' in X.columns:\n",
    "    # drop raw date to avoid leakage through ordering; engineered features kept\n",
    "    drop_cols.append('date')\n",
    "\n",
    "numeric_cols = [c for c in all_cols if c not in set(categorical_cols + drop_cols)]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "    (\"encoder\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Numeric cols (\", len(numeric_cols), \"):\", numeric_cols[:20], '...')\n",
    "print(\"Categorical cols (\", len(categorical_cols), \"):\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780d6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled training split to 250,000 rows for speed\n",
      "Split: time-based split by date, valid_date=2023-11-10\n",
      "Preprocessing fit+transform in 2.80s; Shapes: tr=(250000, 33), va=(7026, 33), full=(667264, 33), test=(20657, 33)\n",
      "Preprocessing fit+transform in 2.80s; Shapes: tr=(250000, 33), va=(7026, 33), full=(667264, 33), test=(20657, 33)\n"
     ]
    }
   ],
   "source": [
    "# 6) Train/Validation Split (date-aware)\n",
    "# If multiple dates, use the latest date as validation; else k-fold fallback\n",
    "if 'date' in X.columns and X['date'].nunique() > 1:\n",
    "    last_date = X['date'].max()\n",
    "    train_mask = X['date'] < last_date\n",
    "    valid_mask = X['date'] == last_date\n",
    "    X_tr, X_va = X.loc[train_mask].copy(), X.loc[valid_mask].copy()\n",
    "    y_tr, y_va = y.loc[train_mask, 'p0q0'].values, y.loc[valid_mask, 'p0q0'].values\n",
    "    split_desc = f\"time-based split by date, valid_date={last_date.date()}\"\n",
    "else:\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X, y['p0q0'].values, test_size=0.1, random_state=RANDOM_STATE)\n",
    "    split_desc = \"random 90/10 split (no multiple dates)\"\n",
    "\n",
    "# Optional downsampling of training rows for speed if extremely large\n",
    "MAX_TRAIN_ROWS = 250_000  # cap to ensure quick training while keeping good signal\n",
    "if len(X_tr) > MAX_TRAIN_ROWS:\n",
    "    idx = np.random.RandomState(RANDOM_STATE).choice(len(X_tr), size=MAX_TRAIN_ROWS, replace=False)\n",
    "    X_tr = X_tr.iloc[idx]\n",
    "    y_tr = y_tr[idx]\n",
    "    print(f\"Downsampled training split to {len(X_tr):,} rows for speed\")\n",
    "\n",
    "print(\"Split:\", split_desc)\n",
    "\n",
    "with Timer() as t:\n",
    "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
    "    Xt_va = preprocessor.transform(X_va)\n",
    "    Xt_full = preprocessor.fit_transform(X)  # for refit later\n",
    "    Xt_test = preprocessor.transform(X_test)\n",
    "print(f\"Preprocessing fit+transform in {t.dt:.2f}s; Shapes: tr={Xt_tr.shape}, va={Xt_va.shape}, full={Xt_full.shape}, test={Xt_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052093a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['hgb', 'rf', 'et', 'ridge', 'lgbm']\n"
     ]
    }
   ],
   "source": [
    "# 7) Define Fast Base Models (HGB, RF, ET, Ridge, optional LGBM)\n",
    "models = []\n",
    "# HistGradientBoosting - strong and fast\n",
    "models.append((\n",
    "    'hgb',\n",
    "    HistGradientBoostingRegressor(\n",
    "        loss='absolute_error',\n",
    "        learning_rate=0.06,\n",
    "        max_depth=6,\n",
    "        max_iter=200,  # keep tight for speed\n",
    "        early_stopping=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "# Random Forest - solid baseline\n",
    "models.append((\n",
    "    'rf',\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=200,   # reduced for speed\n",
    "        max_depth=12,       # reduced depth\n",
    "        n_jobs=-1,\n",
    "        bootstrap=False,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "# Extra Trees - diverse ensemble member\n",
    "models.append((\n",
    "    'et',\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=200,   # reduced for speed\n",
    "        max_depth=14,       # moderate depth\n",
    "        n_jobs=-1,\n",
    "        bootstrap=False,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "))\n",
    "# Ridge - linear anchor\n",
    "models.append((\n",
    "    'ridge',\n",
    "    Ridge(alpha=2.0, random_state=RANDOM_STATE)\n",
    "))\n",
    "\n",
    "if HAS_LGBM:\n",
    "    models.append((\n",
    "        'lgbm',\n",
    "        lgb.LGBMRegressor(\n",
    "            n_estimators=1500,  # rely on early stopping\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.2,\n",
    "            random_state=RANDOM_STATE,\n",
    "            objective='l1'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "print(\"Models:\", [name for name,_ in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37be936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hgb fit in 5.33s, valid MAE=1.4205\n",
      "Model rf fit in 45.82s, valid MAE=1.7267\n",
      "Model rf fit in 45.82s, valid MAE=1.7267\n",
      "Model et fit in 27.22s, valid MAE=1.5084\n",
      "Model ridge fit in 0.08s, valid MAE=1.5388\n",
      "Model et fit in 27.22s, valid MAE=1.5084\n",
      "Model ridge fit in 0.08s, valid MAE=1.5388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1907\n",
      "[LightGBM] [Info] Number of data points in the train set: 250000, number of used features: 32\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1907\n",
      "[LightGBM] [Info] Number of data points in the train set: 250000, number of used features: 32\n",
      "Model lgbm fit in 13.78s, valid MAE=1.3514\n",
      "All models trained in 92.23s\n",
      "Model lgbm fit in 13.78s, valid MAE=1.3514\n",
      "All models trained in 92.23s\n"
     ]
    }
   ],
   "source": [
    "# 8) Fit each model and get validation predictions\n",
    "val_preds = {}\n",
    "fit_models = {}\n",
    "\n",
    "with Timer() as t:\n",
    "    for name, model in models:\n",
    "        with Timer() as tm:\n",
    "            if name == 'lgbm' and HAS_LGBM:\n",
    "                # small early stopping split\n",
    "                model.fit(\n",
    "                    Xt_tr, y_tr,\n",
    "                    eval_set=[(Xt_va, y_va)],\n",
    "                    eval_metric='l1',\n",
    "                    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(Xt_tr, y_tr)\n",
    "            pred = model.predict(Xt_va)\n",
    "        mae = mean_absolute_error(y_va, pred)\n",
    "        val_preds[name] = pred\n",
    "        fit_models[name] = model\n",
    "        print(f\"Model {name} fit in {tm.dt:.2f}s, valid MAE={mae:.4f}\")\n",
    "\n",
    "print(f\"All models trained in {t.dt:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a585b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ensemble MAE= 1.3514\n",
      "Selected weights: {'hgb': 0.0, 'rf': 0.0, 'et': 0.0, 'ridge': 0.0, 'lgbm': 1.0}\n",
      "MAE definition: MAE = (1/n) * sum(|y_i - yhat_i|)\n"
     ]
    }
   ],
   "source": [
    "# 9) Blend weights via small simplex grid search\n",
    "from itertools import product\n",
    "\n",
    "model_names = list(val_preds.keys())\n",
    "P = np.vstack([val_preds[m] for m in model_names]).T  # shape (n, M)\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_w = None\n",
    "best_combo = None\n",
    "\n",
    "steps = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "for w in product(steps, repeat=len(model_names)):\n",
    "    if abs(sum(w) - 1.0) > 1e-9:\n",
    "        continue\n",
    "    y_blend = (P * np.array(w)).sum(axis=1)\n",
    "    mae = mean_absolute_error(y_va, y_blend)\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_w = np.array(w)\n",
    "        best_combo = dict(zip(model_names, w))\n",
    "\n",
    "print(\"Best ensemble MAE=\", round(best_mae, 4))\n",
    "print(\"Selected weights:\", best_combo)\n",
    "\n",
    "# MAE formula for reporting\n",
    "print(\"MAE definition: MAE = (1/n) * sum(|y_i - yhat_i|)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b937535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refit hgb in 8.45s\n",
      "Refit rf in 221.90s\n",
      "Refit rf in 221.90s\n",
      "Refit et in 174.70s\n",
      "Refit ridge in 0.16s\n",
      "Refit et in 174.70s\n",
      "Refit ridge in 0.16s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1950\n",
      "[LightGBM] [Info] Number of data points in the train set: 667264, number of used features: 32\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1950\n",
      "[LightGBM] [Info] Number of data points in the train set: 667264, number of used features: 32\n",
      "Refit lgbm in 70.48s\n",
      "Refit all models in 475.69s\n",
      "Refit lgbm in 70.48s\n",
      "Refit all models in 475.69s\n"
     ]
    }
   ],
   "source": [
    "# 10) Retrain base models on full train\n",
    "fit_models_full = {}\n",
    "with Timer() as t:\n",
    "    for name, model in models:\n",
    "        with Timer() as tm:\n",
    "            if name == 'lgbm' and HAS_LGBM:\n",
    "                model.set_params(n_estimators=2000, learning_rate=0.025)\n",
    "                model.fit(\n",
    "                    Xt_full, y['p0q0'].values,\n",
    "                    eval_set=[(Xt_va, y_va)],\n",
    "                    eval_metric='l1',\n",
    "                    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(Xt_full, y['p0q0'].values)\n",
    "        fit_models_full[name] = model\n",
    "        print(f\"Refit {name} in {tm.dt:.2f}s\")\n",
    "print(f\"Refit all models in {t.dt:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fd66e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference + post-process in 1.19s. Pred range before rounding: [-3.00, 3.02] -> after: [-3, 3] with clip to [-160,15]\n"
     ]
    }
   ],
   "source": [
    "# 11) Inference on Test, Post-process\n",
    "with Timer() as t:\n",
    "    preds_test_members = {}\n",
    "    for name, model in fit_models_full.items():\n",
    "        preds_test_members[name] = model.predict(Xt_test)\n",
    "    # Blend\n",
    "    order = list(fit_models_full.keys())\n",
    "    w = np.array([best_combo.get(m, 0.0) for m in order])\n",
    "    w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "    Ptest = np.vstack([preds_test_members[m] for m in order]).T\n",
    "    y_test_pred = (Ptest * w).sum(axis=1)\n",
    "\n",
    "    # Round to integer and optional clip\n",
    "    y_min, y_max = int(np.floor(y['p0q0'].min())), int(np.ceil(y['p0q0'].max()))\n",
    "    do_clip = True\n",
    "    y_submit = np.rint(y_test_pred)\n",
    "    if do_clip:\n",
    "        y_submit = np.clip(y_submit, y_min, y_max)\n",
    "\n",
    "print(f\"Inference + post-process in {t.dt:.2f}s. Pred range before rounding: [{y_test_pred.min():.2f}, {y_test_pred.max():.2f}] -> after: [{y_submit.min():.0f}, {y_submit.max():.0f}] with clip to [{y_min},{y_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93bddb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to: \\\\wsl.localhost\\Ubuntu-24.04\\home\\utharushan\\ChallengeData\\SNCF-Transilien-challenge\\y_test.csv\n",
      "   p0q0\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n"
     ]
    }
   ],
   "source": [
    "# 12) Write Submission File y_test.csv\n",
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(data_dir) / 'y_test.csv'\n",
    "sub = pd.DataFrame({'p0q0': y_submit.astype(int)})\n",
    "sub.to_csv(out_path, index=True, index_label='id')\n",
    "print(f\"Wrote submission to: {out_path}\")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid MAE hgb: 1.4205\n",
      "Valid MAE rf: 1.7267\n",
      "Valid MAE et: 1.5084\n",
      "Valid MAE ridge: 1.5388\n",
      "Valid MAE lgbm: 1.3514\n",
      "Ensemble MAE: 1.3514\n",
      "Weights: {'hgb': 0.0, 'rf': 0.0, 'et': 0.0, 'ridge': 0.0, 'lgbm': 1.0}\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 13) Assertions and runtime reporting\n",
    "assert len(sub) == len(X_test), (len(sub), len(X_test))\n",
    "assert np.allclose(sub['p0q0'].values, np.rint(sub['p0q0']).values), \"Submission must be integers after rounding\"\n",
    "\n",
    "# Print per-model MAE and ensemble\n",
    "for name, pred in val_preds.items():\n",
    "    print(f\"Valid MAE {name}: {mean_absolute_error(y_va, pred):.4f}\")\n",
    "print(f\"Ensemble MAE: {best_mae:.4f}\")\n",
    "print(\"Weights:\", best_combo)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
